{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Project: Winning Jeopardy**\n",
    "\n",
    "Jeopardy is a popular TV show in the US where participants answer questions to win money. It's been running for many years, and is a major force in popular culture.\n",
    "\n",
    "The dataset is named jeopardy.csv, and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which you can download [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from random import choice\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def normalize_values(text):\n",
    "    text = re.sub(\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "    try:\n",
    "        text = int(text)\n",
    "    except Exception:\n",
    "        text = 0\n",
    "    return text\n",
    "\n",
    "def count_matches(row):\n",
    "    split_answer = row[\"clean_answer\"].split()\n",
    "    split_question = row[\"clean_question\"].split()\n",
    "    if \"the\" in split_answer:\n",
    "        split_answer.remove(\"the\")\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    match_count = 0\n",
    "    for item in split_answer:\n",
    "        if item in split_question:\n",
    "            match_count += 1\n",
    "    return match_count / len(split_answer)\n",
    "\n",
    "def determine_value(row, col = 'clean_value'):\n",
    "    value = 0\n",
    "    if row[col] > 800:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "def count_usage(term, df, col_1 = 'clean_question', col_2 = 'high_value'):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if term in row[col_1].split(\" \"):\n",
    "            if row[col_2] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return high_count, low_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv('jeopardy.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove spaces in col names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# clean question / answer / val data\n",
    "df[\"clean_question\"] = df[\"Question\"].apply(normalize_text)\n",
    "df[\"clean_answer\"] = df[\"Answer\"].apply(normalize_text)\n",
    "df[\"clean_value\"] = df[\"Value\"].apply(normalize_values)\n",
    "\n",
    "# format date\n",
    "df['Air Date'] = pd.to_datetime(df['Air Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "- How often the answer can be used for a question.\n",
    "- How often questions are repeated.\n",
    "\n",
    "You can answer the second question by seeing how often complex words (> 6 characters) reoccur. You can answer the first question by seeing how many times words in the answer also occur in the question. We'll work on the first question and come back to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05900196524977763"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"answer_in_question\"] = df.apply(count_matches, axis=1)\n",
    "df[\"answer_in_question\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the answer only makes up 6% of the question. \n",
    "\n",
    "This isn't a huge number, and means that we probably can't just hope that hearing a question will enable us to figure out the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6876260592169802"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "\n",
    "df = df.sort_values(\"Air Date\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "        split_question = row[\"clean_question\"].split(\" \")\n",
    "        split_question = [q for q in split_question if len(q) > 5]\n",
    "        \n",
    "        match_count = 0\n",
    "        \n",
    "        for word in split_question:\n",
    "            if word in terms_used:\n",
    "                match_count += 1\n",
    "        \n",
    "        for word in split_question:\n",
    "            terms_used.add(word)\n",
    "        \n",
    "        if len(split_question) > 0:\n",
    "            match_count /= len(split_question)\n",
    "        \n",
    "        question_overlap.append(match_count)\n",
    "        \n",
    "df[\"question_overlap\"] = question_overlap\n",
    "\n",
    "df[\"question_overlap\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's about 70% overlap between terms in new questions and terms in old questions. \n",
    "\n",
    "This only looks at a small set of questions, and it doesn't look at phrases, it looks at single terms. \n",
    "\n",
    "This makes it relatively insignificant, but it does mean that it's worth looking more into the recycling of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"high_value\"] = df.apply(determine_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (5, 4),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (1, 0),\n",
       " (2, 1),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (0, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_used_list = list(terms_used)\n",
    "comparison_terms = [choice(terms_used_list) for _ in range(10)]\n",
    "\n",
    "observed_expected = []\n",
    "\n",
    "for term in comparison_terms:\n",
    "    observed_expected.append(count_usage(term, df))\n",
    "\n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=3.180689829769113, pvalue=0.07451326597343068),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.803925692253768, pvalue=0.3699222378079571),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=2.1177104383031944, pvalue=0.14560406868264344),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.803925692253768, pvalue=0.3699222378079571),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_value_count = df[df[\"high_value\"] == 1].shape[0]\n",
    "low_value_count = df[df[\"high_value\"] == 0].shape[0]\n",
    "\n",
    "chi_squared = []\n",
    "for obs in observed_expected:\n",
    "    total = sum(obs)\n",
    "    total_prop = total / df.shape[0]\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([obs[0], obs[1]])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "\n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the terms had a significant difference in usage between high value and low value rows. \n",
    "\n",
    "Additionally, the frequencies were all lower than 5, so the chi-squared test isn't as valid. \n",
    "\n",
    "It would be better to run this test with only terms that have higher frequencies.\n",
    "\n",
    "\n",
    "**Potential next steps:**\n",
    "\n",
    "- Find a better way to eliminate non-informative words than just removing words that are less than 6 characters long. Some ideas:\n",
    "    - Manually create a list of words to remove, like the, than, etc.\n",
    "    - Find a list of stopwords to remove.\n",
    "    - Remove words that occur in more than a certain percentage (like 5%) of questions.\n",
    "- Perform the chi-squared test across more terms to see what terms have larger differences. This is hard to do currently because the code is slow, but here are some ideas:\n",
    "    - Use the apply method to make the code that calculates frequencies more efficient.\n",
    "    - Only select terms that have high frequencies across the dataset, and ignore the others.\n",
    "- Look more into the Category column and see if any interesting analysis can be done with it. Some ideas:\n",
    "    - See which categories appear the most often.\n",
    "    - Find the probability of each category appearing in each round.\n",
    "- Use the whole Jeopardy dataset instead of the subset we used in this lesson.\n",
    "- Use phrases instead of single words when seeing if there's overlap between questions. Single words don't capture the whole context of the question well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
